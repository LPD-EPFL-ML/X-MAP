{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_root = \"/home/tlin/notebooks/data\"\n",
    "path_pickle_source = join(\n",
    "    path_root, \"cache/two_domain/clean_data/movie\")\n",
    "path_pickle_target = join(\n",
    "    path_root, \"cache/two_domain/clean_data/book\")\n",
    "path_pickle_train = join(\n",
    "    path_root, \"cache/two_domain/split_data/train\")\n",
    "path_pickle_test = join(\n",
    "    path_root, \"cache/two_domain/split_data/test\")\n",
    "path_pickle_baseline_sim = join(\n",
    "    path_root, \"cache/two_domain/item_based_sim/base_sim\")\n",
    "path_pickle_extended_sim = join(\n",
    "    path_root, \"cache/two_domain/extend_sim/extendsim\")\n",
    "path_pickle_private_mapped_sim = join(\n",
    "    path_root, \"cache/two_domain/private_mapping/privatemap\")\n",
    "path_pickle_nonprivate_mapped_sim = join(\n",
    "    path_root, \"cache/two_domain/private_mapping/nonprivatemap\")\n",
    "path_pickle_userbased_alterEgo = join(\n",
    "    path_root, \"cache/two_domain/cross_sim/user_based_alterEgo\")\n",
    "path_pickle_itembased_alterEgo = join(\n",
    "    path_root, \"cache/two_domain/cross_sim/item_based_alterEgo\")\n",
    "path_pickle_alterEgo_userbased_sim = join(\n",
    "    path_root, \"cache/two_domain/cross_sim/targetdomain_userbased_sim\")\n",
    "path_pickle_alterEgo_itembased_sim = join(\n",
    "    path_root, \"cache/two_domain/cross_sim/targetdomain_itembased_sim\")\n",
    "path_pickle_private_policy_userbased_sim = join(\n",
    "    path_root, \"cache/two_domain/policy/policy_userbased_sim\")\n",
    "path_pickle_private_policy_itembased_sim = join(\n",
    "    path_root, \"cache/two_domain/policy/policy_itembased_sim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movieRDD = sc.pickleFile(path_pickle_source)\n",
    "bookRDD = sc.pickleFile(path_pickle_target)\n",
    "sourceRDD, targetRDD = movieRDD, bookRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_left = 0\n",
    "ratio_split = 0.2\n",
    "ratio_both = 0.8\n",
    "seed = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def find_overlap_user(sourceRDD, targetRDD):\n",
    "        \"\"\"Find overlap users.\n",
    "        return a RDD of uid.\n",
    "        \"\"\"\n",
    "        return sourceRDD.keys().intersection(targetRDD.keys())\n",
    "    \n",
    "overlap_userRDD = find_overlap_user(sourceRDD, targetRDD)\n",
    "overlap_userRDD_bd = sc.broadcast(overlap_userRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def distinguish_data(overlap_userRDD_bd, sourceRDD, targetRDD):\n",
    "        \"\"\"Distinguish data from overlap data to non-overlap data.\"\"\"\n",
    "        def in_split(iterators):\n",
    "            for line in iterators:\n",
    "                if line[0] in overlap_userRDD_bd.value:\n",
    "                    yield line\n",
    "\n",
    "        def out_split(iterators):\n",
    "            for line in iterators:\n",
    "                if line[0] not in overlap_userRDD_bd.value:\n",
    "                    yield line\n",
    "\n",
    "        overlap_sourceRDD = sourceRDD.mapPartitions(in_split)\n",
    "        non_overlap_sourceRDD = sourceRDD.mapPartitions(out_split)\n",
    "        overlap_targetRDD = targetRDD.mapPartitions(in_split)\n",
    "        non_overlap_targetRDD = targetRDD.mapPartitions(out_split)\n",
    "\n",
    "        return non_overlap_sourceRDD, overlap_sourceRDD, \\\n",
    "            non_overlap_targetRDD, overlap_targetRDD\n",
    "        \n",
    "non_overlap_sourceRDD, overlap_sourceRDD, \\\n",
    "    non_overlap_targetRDD, overlap_targetRDD = distinguish_data(\n",
    "        overlap_userRDD_bd, sourceRDD, targetRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unionRDD = overlap_sourceRDD.union(\n",
    "            overlap_targetRDD).reduceByKey(lambda a, b: a + b)\n",
    "splits = unionRDD.randomSplit(\n",
    "            [0.2, 0.8,\n",
    "             1 - 0.2 - 0.8],\n",
    "            seed=1)\n",
    "splits[0].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    import random\n",
    "    def determine_remaining(iterators):\n",
    "        \"\"\"split test dataset into (source, remain, remove).\n",
    "        Args:\n",
    "            iterators: (uid, lines), where line=[(iid, rating, time)*]\n",
    "        Returns:\n",
    "            uid, source data, remaining data(target), removed data(target)\n",
    "            The remaining data can be empty.\n",
    "        \"\"\"\n",
    "        def decide_remaining(target):\n",
    "            remaining_dict = random.sample(target, 0)\n",
    "            test_dict = [\n",
    "                item for item in target if item not in remaining_dict]\n",
    "            return [remaining_dict, test_dict]\n",
    "\n",
    "        for uid, lines in iterators:\n",
    "            source = [line for line in lines if \"S:\" in line[0]]\n",
    "            target = filter(lambda line: line not in source, lines)\n",
    "            remain, remove = decide_remaining(target)\n",
    "            yield uid, source, remain, remove\n",
    "\n",
    "\n",
    "test_part_dataRDD = splits[0].mapPartitions(determine_remaining).cache()\n",
    "uid, lines = splits[0].take(1)[0]\n",
    "source = [line for line in lines if \"S:\" in line[0]]\n",
    "target = filter(lambda line: line not in source, lines)\n",
    "remaining_dict = random.sample(list(target), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
