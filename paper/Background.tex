\vspace{-2mm}
\section{Preliminaries}
\label{background}
In this section, we present some preliminaries on collaborative filtering before formulating the heterogeneous recommendation problem.
%In this section, we recall some background on collaborative filtering before formulating the heterogeneous recommendation problem. 
% used in \crossrec followed by the problem formulation 
% along with the notations. 
Table~\ref{fig:NotationTable} summarizes some notations we use in this paper.

\vspace{-1mm}
\subsection{Collaborative Filtering}

\emph{Collaborative Filtering} (CF) algorithms fall mainly in two categories: \emph{memory-based}~\cite{breese1998empirical,resnick1994grouplens, soboroff2000collaborative} and \emph{model-based}~\cite{hofmann1999latent, kohrs1999clustering, ungar1998clustering}. Memory-based algorithms compute the \emph{top-k} like-minded users for a given user (Alice), denoted as the \emph{neighbors} of Alice, from the training database and make recommendations to Alice based on the rating history of her neighbors. In contrast to memory-based algorithms, model-based ones first extract some information about users (including Alice) from the database to train a \emph{model} and then use this model to make recommendations for the users (including Alice).
Memory-based algorithms have two advantages over model-based ones. First, new information, like recent ratings by neighbors, can be easily integrated in the recommender. Second, item contents or features need not be considered in the prediction, leading to less computations.

%In contrast to memory-based algorithms, model-based algorithms group users in the training database into a pre-defined number of classes based on their rating histories. Then, users are classified into one of these pre-defined classes and then the (model-based) algorithm computes the predictions based on the ratings in the classified class. 

Among memory-based CF schemes, \emph{neighbor-based CF}, namely \emph{k nearest neighbor} (kNN) algorithms are more popular and widely used in practice~\cite{sarwar2001item}. A user-based CF scheme, depicted in Algorithm~\ref{ub_cf}, computes the $k$ neighbors of Alice with highest user-to-user similarities (Phase 1 in Algorithm~\ref{ub_cf}) and then these neighbors' profiles are leveraged to compute the best recommendations for Alice (Phase 2 in Algorithm~\ref{ub_cf}). An item-based CF scheme, depicted in Algorithm~\ref{ib_cf}, computes the $k$ most similar items for every item based on item-to-item similarities (Phase 1 in Algorithm~\ref{ib_cf}) and then computes the recommendations for Alice based on her ratings for similar items (Phase 2 in Algorithm~\ref{ib_cf}). {\color{red}\crossrec currently supports both user-based and item-based CF schemes for providing recommendations to its users.}
\vspace{2mm}


%An item-based kNN scheme also follows similar computation steps. \crossrec supports both user-based and item-based kNN schemes.

\begin{algorithm}[ht]
\caption{User-based CF}\label{ub_cf}
\begin{algorithmic}[1]
\Require $\mathcal{I}$: Set of all items; $\mathcal{U}$: Set of all users; $X$: set of profiles of all users where $X_A$ denotes the profile of Alice (with user-id as $A$) which contains the items rated by $A$.
\Ensure $R_A$: Top-$N$ recommendations for Alice
\Algphase{Phase 1 - Nearest Neighbor Selection: kNN($A$,$\mathcal{U}$,$X$)}
\Require $\mathcal{U}$, $X$
\Ensure $k$ nearest neighbors for Alice
\State var $\tau_A$;		\hfill $\rhd$ Dictionary with similarities for Alice
\State var $\bar{r}$; \hfill $\rhd$ Average rating for each item
\For {$u$ in $\mathcal{U}$}
\State \vspace{-4mm}
\begin{equation}
\vspace{-2mm}
\label{tau_ub_cf}
\tau_A[u]=\frac{\sum _{i \in X_A \cap X_u} (r_{A,i} - \bar{r_i})(r_{u,i} - \bar{r_i})}{\sqrt{\sum _{i \in X_A} (r_{A,i} - \bar{r_i})^2}\sqrt{\sum _{i \in X_u} (r_{u,i} - \bar{r_i})^2}}
\end{equation}
\EndFor
\State $N_{A}=\tau_A.sortByValue(ascending=false);$
\State \textbf{return:} $N_{A}[:k]$;
\Algphase{Phase 2 - Recommendation: Top-N($N_A$,$\mathcal{I}$)}
\Require $\mathcal{I}$, $N_A$ 
\Ensure Top-$N$ recommendations for Alice
\State var Pred; \hfill $\rhd$ Dictionary with predictions for Alice
\State var $\bar{r}$; \hfill $\rhd$ Average rating for each user
\For {$i$ : $item$ in $\mathcal{I}$}
\State \vspace{-4mm}
\begin{equation}
\vspace{-2mm}
\label{pred_ub_cf}
Pred[i]=\bar{r}_{A} +\frac{\sum_{B \in N_{A}} \tau(A,B) \cdot (r_{B,i}- \bar{r}_B)}{\sum_{B \in N_{A}} |\tau(A,B)| } ;
\end{equation}
\EndFor
\State $R_{A}=Pred.sortByValue(ascending=false);$
\State \textbf{return:} $R_{A}[:N]$;
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{Item-based CF}\label{ib_cf}
\begin{algorithmic}[1]
\Require $\mathcal{I}$: Set of all items; $\mathcal{U}$: Set of all users; $Y$: Set of profiles of all items where $Y_j$ denotes the list of users who rated an item with item-id $j$.
\Ensure $R_A$: Top-$N$ recommendations for Alice ($A$)
\Algphase{Phase 1 - Nearest Neighbor Selection: kNN($j$,$\mathcal{I}$,$Y$)}
\Require $\mathcal{I}$, $Y$
\Ensure $k$ most similar items to item $j$
\State var $\tau_j$;		\hfill $\rhd$ Dictionary for item similarities with $j$
\State var $\bar{r}$; \hfill $\rhd$ Average rating for each user
\For {$i$ in $\mathcal{I}$}
\State \vspace{-4mm}
\begin{equation}
\vspace{-2mm}
\label{tau_ib_cf}
\tau_j[i]=\frac{\sum _{u \in Y_j \cap Y_i} (r_{u,j} - \bar{r_u})(r_{u,i} - \bar{r_u})}{\sqrt{\sum _{u \in Y_j} (r_{u,j} - \bar{r_u})^2}\sqrt{\sum _{u \in Y_i} (r_{u,i} - \bar{r_u})^2}}
\end{equation}
\EndFor
\State $N_{j}=\tau_j.sortByValue(ascending=false);$
\State \textbf{return:} $N_{j}[:k]$;
\Algphase{Phase 2 - Recommendation: Top-N($Y$,$\mathcal{I}$)}
\Require $\mathcal{I}$, $Y$
\Ensure Top-$N$ recommendations for Alice
\State var Pred; \hfill $\rhd$ Dictionary with predictions for Alice
\State var $\bar{r}$; \hfill $\rhd$ Average rating for each item
\For {$i$ : $item$ in $\mathcal{I}$}
\State \vspace{-4mm}
\begin{equation}
\vspace{-2mm}
\label{pred_ib_cf}
Pred[i]=\bar{r}_{i} +\frac{\sum_{j \in N_{i}} \tau(i,j) \cdot (r_{A,j}- \bar{r}_j)}{\sum_{j \in N_{i}} |\tau(i,j)| } ;
\end{equation}
\EndFor
\State $R_{A}=Pred.sortByValue(ascending=false);$
\State \textbf{return:} $R_{A}[:N]$;
\end{algorithmic}
\end{algorithm}

\vspace{-4mm}

\input{Table}
\subsection{Problem Formulation}
Without loss of generality, we assume two domains referred to as the {\textit{source}} domain ($\mathcal{D}^S$) and the {\textit{target}} domain ($\mathcal{D}^T$). 
We use superscript notations $^S$ and $^T$ to differentiate the source and the target
domains. We assume that users in $\mathcal{U}^S$ and $\mathcal{U}^T$ overlaps, but $\mathcal{I}^S$ and $\mathcal{I}^T$ have no common items. This captures the most common heterogeneous personalization scenario in web companies nowadays, e.g. Amazon, E-bay. We now define the recommendation problem which we study in this paper as follows.
\begin{definition}
Given the source domain $\mathcal{D}^S$ and the target domain $\mathcal{D}^T$, the heterogeneous personalization problem consists in recommending items in $\mathcal{I}^T$ to users in $\mathcal{U}^S$ based on the preferences of $\mathcal{U}^S$ for $\mathcal{I}^S$, $\mathcal{U}^T$ for $\mathcal{I}^T$ and $\mathcal{U}^S \cap \mathcal{U}^T$ for $\mathcal{I}^S \cup \mathcal{I}^T$. More precisely, we aim to recommend items in $\mathcal{I}^T$ to a user who rated a few items (sparsity) or no items (cold-start) in $\mathcal{I}^T$.
\end{definition}

The scenario, underlying Figure~\ref{fig:overlap}, illustrates this problem. The goal is to recommend new relevant items from $\mathcal{D}^T$ (e.g., books) either to Alice who never rated any book (cold-start) or to Bob who rated only a single book (sparsity). Both rated items in $\mathcal{D}^S$ (e.g., movies).
